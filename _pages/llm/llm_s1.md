---
layout: archive
title: "LLM Section 1"
permalink: /llm/s1/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}



# 第一章 大模型简介

国外知名LLM：GPT-3.5、GPT-4、PaLM、Claude 和 LLaMA 等。
国内知名LLM：文心一言、讯飞星火、通义千问、ChatGLM、百川等。

大语言模型LLM相比于小型语言模型，在在解决复杂任务时表现出了惊人的潜力，这被称为“**涌现能力**”。以 GPT-3 和 GPT-2 为例，GPT-3 可以通过学习上下文来解决少样本任务。


## GPT
GPT 模型的基本原则是通过语言建模将世界知识压缩到仅解码器 **(decoder-only)** 的 Transformer 模型中，这样它就可以恢复(或记忆)世界知识的语义，并充当通用任务求解器。它能够成功的两个关键点：
- 训练能够准确预测下一个单词的 decoder-only 的 Transformer 语言模型
- 扩展语言模型的大小

## 开源LLM——LLaMa
LLaMA 系列模型是 Meta 开源的一组参数规模 从 7B 到 70B 的基础语言模型。
它们都是在数万亿个字符上训练的，展示了如何仅使用公开可用的数据集来训练最先进的模型，而不需要依赖专有或不可访问的数据集。
LLaMA 模型使用了大规模的数据过滤和清洗技术，以提高数据质量和多样性，减少噪声和偏见。
LLaMA 模型还使用了高效的数据并行和流水线并行技术，以加速模型的训练和扩展。

与 GPT 系列相同，LLaMA 模型也采用了 decoder-only 架构，同时结合了一些前人工作的改进：

- Pre-normalization 正则化：为了提高训练稳定性，LLaMA 对每个 Transformer 子层的输入进行了 RMSNorm 归一化，这种归一化方法可以避免梯度爆炸和消失的问题，提高模型的收敛速度和性能；
- SwiGLU 激活函数：将 ReLU 非线性替换为 SwiGLU 激活函数，增加网络的表达能力和非线性，同时减少参数量和计算量；
- 旋转位置编码（RoPE，Rotary Position Embedding）：模型的输入不再使用位置编码，而是在网络的每一层添加了位置编码，RoPE 位置编码可以有效地捕捉输入序列中的相对位置信息，并且具有更好的泛化能力。

## 其他开源LLM
1. 通义千问由阿里巴巴基于“通义”大模型研发，于 2023 年 4 月正式发布。2023 年 9 月，阿里云开源了 Qwen（通义千问）系列工作。2024 年 2 月 5 日，开源了 Qwen1.5（Qwen2 的测试版）。并于 2024 年 6 月 6 日正式开源了 Qwen2。 Qwen2 是一个 decoder-Only 的模型，采用 SwiGLU 激活、RoPE、GQA的架构。中文能力相对来说非常不错的开源模型。
2. GLM 系列模型是清华大学和智谱 AI 等合作研发的语言大模型。2023 年 3 月 发布了 ChatGLM。
3. Baichuan 是由百川智能开发的开源可商用的语言大模型。其基于Transformer 解码器架构（decoder-only）。

## LLM能力与特点
### 涌现能力（emergent abilities）
涌现能力：通俗来讲就是量变引起质变，模型性能随着规模增大而迅速提升，超过了随机水平。

LLM三个典型涌现能力：
- 上下文学习：该能力允许语言模型在提供自然语言指令或多个任务示例的情况下，通过理解上下文并生成相应输出的方式来执行任务，而无需额外的训练或参数更新。
- 指令遵循：通过使用自然语言描述的多任务数据进行微调，也就是所谓的 指令微调。
- 逐步推理：LLM 通过采用 思维链（CoT, Chain of Thought） 推理策略，利用包含中间推理步骤的提示机制来解决这些任务，从而得出最终答案。

### 作为基座模型指出多元应用的能力
基座模型（foundation model）：这是一种全新的 AI 技术范式，借助于海量无标注数据的训练，获得可以适用于大量下游任务的大模型（单模态或者多模态）。这样，多个应用可以只依赖于一个或少数几个大模型进行统一建设。

### 支持对话作为统一入口的能力


## LLM的特点
预训练和微调： LLM 采用了预训练和微调的学习方法。首先在大规模文本数据上进行预训练（无标签数据），学习通用的语言表示和知识。然后通过微调（有标签数据）适应特定任务，从而在各种 NLP 任务中表现出色。


## LLM的应用与影响

## 检索增强生成RAG
大型语言模型（LLM）相较于传统的语言模型具有更强大的能力，然而在某些情况下，它们仍可能无法提供准确的答案。为了解决大型语言模型在生成文本时面临的一系列挑战，提高模型的性能和输出质量，研究人员提出了一种新的模型架构：检索增强生成（RAG, Retrieval-Augmented Generation）。该架构巧妙地整合了从庞大知识库中检索到的相关信息，并以此为基础，指导大型语言模型生成更为精准的答案，从而显著提升了回答的准确性与深度。

目前 LLM 面临的主要问题有：
- 信息偏差/幻觉： LLM 有时会产生与客观事实不符的信息，导致用户接收到的信息不准确。RAG 通过检索数据源，辅助模型生成过程，确保输出内容的精确性和可信度，减少信息偏差。
- 知识更新滞后性： LLM 基于静态的数据集训练，这可能导致模型的知识更新滞后，无法及时反映最新的信息动态。RAG 通过实时检索最新数据，保持内容的时效性，确保信息的持续更新和准确性。
- 内容不可追溯： LLM 生成的内容往往缺乏明确的信息来源，影响内容的可信度。RAG 将生成内容与检索到的原始资料建立链接，增强了内容的可追溯性，从而提升了用户对生成内容的信任度。
- 领域专业知识能力欠缺： LLM 在处理特定领域的专业知识时，效果可能不太理想，这可能会影响到其在相关领域的回答质量。RAG 通过检索特定领域的相关文档，为模型提供丰富的上下文信息，从而提升了在专业领域内的问题回答质量和深度。
- 推理能力限制： 面对复杂问题时，LLM 可能缺乏必要的推理能力，这影响了其对问题的理解和回答。RAG 结合检索到的信息和模型的生成能力，通过提供额外的背景知识和数据支持，增强了模型的推理和理解能力。
- 应用场景适应性受限： LLM 需在多样化的应用场景中保持高效和准确，但单一模型可能难以全面适应所有场景。RAG 使得 LLM 能够通过检索对应应用场景数据的方式，灵活适应问答系统、推荐系统等多种应用场景。
- 长文本处理能力较弱： LLM 在理解和生成长篇内容时受限于有限的上下文窗口，且必须按顺序处理内容，输入越长，速度越慢。RAG 通过检索和整合长文本信息，强化了模型对长上下文的理解和生成，有效突破了输入长度的限制，同时降低了调用成本，并提升了整体的处理效率。

RAG工作流程：
1. 数据处理阶段
   1. 对原始数据进行清洗和处理。
   2. 将处理后的数据转化为检索模型可以使用的格式。
   3. 将处理后的数据存储在对应的数据库中。
2. 检索阶段
   1. 将用户的问题输入到检索系统中，从数据库中检索相关信息。
3. 增强阶段
   1. 对检索到的信息进行处理和增强，以便生成模型可以更好地理解和使用。
4. 生成阶段
   1. 将增强后的信息输入到生成模型中，生成模型根据这些信息生成答案。

在提升大语言模型效果中，RAG 和 微调（Finetune）是两种主流的方法。

微调: 通过在特定数据集上进一步训练大语言模型，来提升模型在特定任务上的表现。

RAG 和 微调的对比可以参考下表

特征比较	RAG	微调
知识更新	直接更新检索知识库，无需重新训练。信息更新成本低，适合动态变化的数据。	通常需要重新训练来保持知识和数据的更新。更新成本高，适合静态数据。
外部知识	擅长利用外部资源，特别适合处理文档或其他结构化/非结构化数据库。	将外部知识学习到 LLM 内部。
数据处理	对数据的处理和操作要求极低。	依赖于构建高质量的数据集，有限的数据集可能无法显著提高性能。
模型定制	侧重于信息检索和融合外部知识，但可能无法充分定制模型行为或写作风格。	可以根据特定风格或术语调整 LLM 行为、写作风格或特定领域知识。
可解释性	可以追溯到具体的数据来源，有较好的可解释性和可追踪性。	黑盒子，可解释性相对较低。
计算资源	需要额外的资源来支持检索机制和数据库的维护。	依赖高质量的训练数据集和微调目标，对计算资源的要求较高。
推理延迟	增加了检索步骤的耗时	单纯 LLM 生成的耗时
降低幻觉	通过检索到的真实信息生成回答，降低了产生幻觉的概率。	模型学习特定领域的数据有助于减少幻觉，但面对未见过的输入时仍可能出现幻觉。
伦理隐私	检索和使用外部数据可能引发伦理和隐私方面的问题。	训练数据中的敏感信息需要妥善处理，以防泄露。

## LangChain
利用 OpenAI 提供的 API 或者私有化模型，来开发基于大型语言模型的应用程序。
帮助开发者们快速构建基于大型语言模型的端到端应用程序或工作流程。
LangChain 框架是一个开源工具，充分利用了大型语言模型的强大能力，以便开发各种下游应用。它的目标是为各种大型语言模型应用提供通用接口，从而简化应用程序的开发流程。具体来说，LangChain 框架可以实现数据感知和环境互动，也就是说，它能够让语言模型与其他数据来源连接，并且允许语言模型与其所处的环境进行互动。

利用 LangChain 框架，我们可以轻松地构建如下所示的 RAG 应用（图片来源）。在下图中，每个椭圆形代表了 LangChain 的一个模块，例如数据收集模块或预处理模块。每个矩形代表了一个数据状态，例如原始数据或预处理后的数据。箭头表示数据流的方向，从一个模块流向另一个模块。在每一步中，LangChain 都可以提供对应的解决方案，帮助我们处理各种任务。
[langchain](https://github.com/niysNiysniys/niysNiysniys.github.io/blob/master/images/C1-3-langchain.png" alt="langchain)

## 开发LLM应用整体流程
- 整体思路：用 Prompt Engineering 来替代子模型的训练调优，通过 Prompt 链路组合来实现业务逻辑，用一个通用大模型 + 若干业务 Prompt 来解决任务，从而将传统的模型训练调优转变成了更简单、轻松、低成本的 Prompt 设计调优。
- 评估思路：流程更为灵活和敏捷。从实际业务需求出发构造小批量验证集，设计合理 Prompt 来满足验证集效果。然后，将不断从业务逻辑中收集当下 Prompt 的 Bad Case，并将 Bad Case 加入到验证集中，针对性优化 Prompt，最后实现较好的泛化效果。


流程：
1. 确定目标。
2. 设计功能。
3. 搭建整体架构。
4. 搭建数据库。
   一般使用诸如 Chroma 的向量数据库。在该步骤中，我们需要收集数据并进行预处理，再向量化存储到数据库中。数据预处理一般包括从多种格式向纯文本的转化，例如 PDF、MarkDown、HTML、音视频等，以及对错误数据、异常数据、脏数据进行清洗。完成预处理后，需要进行切片、向量化构建出个性化数据库。
6. Prompt Engineering。
7. 验证迭代。
8. 前后端搭建。
   我们采用 Gradio 和 Streamlit，可以帮助个体开发者迅速搭建可视化页面实现 Demo 上线。
10. 体验优化。
